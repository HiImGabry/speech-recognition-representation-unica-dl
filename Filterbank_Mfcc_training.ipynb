{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97359f5-90dc-48a8-a36b-02a4d99a7b29",
   "metadata": {},
   "source": [
    "# Addestramento modello convolutivo\n",
    "Questo notebook è incentrato sull'addestramento di tutti i modelli presenti nel notebook del modello convolutivo base, con i filterbanks e gli mfcc. Attraverso questo notebook saremo in grado di ottenere prestazioni su dataset differenti, così da poter paragonare le prestazioni dei diversi modelli su dataset diversi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480c543a-b196-43c1-9ae0-b2a1b15ff310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.fftpack as scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfe1f6-6018-470d-815c-1441dcf10724",
   "metadata": {},
   "source": [
    "## Classe per la conversione del dataset\n",
    "Vista la quantità di opzioni che abbiamo abbiamo scelto di creare una classe che preso un dataset di audio in input sia in grado di convertirlo in spettrogrammi, filterbanks, o mfcc. Tutte le funzioni utilizzate all'interno della classe le abbiamo già viste nei notebook specifici dei singoli argomenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a424b5d-76ec-4f49-937e-c3ef68a405b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConverter:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def convert(self, option):\n",
    "        available_options = ['spettrogrammi', 'filterbanks', 'mfcc']\n",
    "        \n",
    "        if option == available_options[0]:\n",
    "            return self.get_spectrogram_dataset()\n",
    "        elif option == available_options[1]:\n",
    "            return self.get_filterbanks_dataset()\n",
    "        elif option == available_options[2]:\n",
    "            return self.get_mfcc_dataset()\n",
    "        else:\n",
    "            raise ValueError(f\"Opzione non disponibile: inserire una delle seguenti opzioni: {available_options}\")\n",
    "    \n",
    "    # INIZIO SPETTROGRAMMI\n",
    "    def squeeze(self, audio, labels):\n",
    "        audio = tf.squeeze(audio, axis=-1)\n",
    "        return audio, labels\n",
    "    \n",
    "    def get_spectrogram(self, waveform):\n",
    "    # applichiamo la short-time Fourier transorm\n",
    "        spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n",
    "        spectrogram = tf.abs(spectrogram)\n",
    "        \n",
    "        return spectrogram[..., tf.newaxis]\n",
    "    \n",
    "    def get_spectrogram_dataset(self):\n",
    "        # squeeze\n",
    "        self.dataset = self.dataset.map(self.squeeze, tf.data.AUTOTUNE)\n",
    "        self.dataset = self.dataset.map(lambda x, y: (self.get_spectrogram(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return self.dataset\n",
    "\n",
    "    # FINE SPETTROGRAMMI\n",
    "\n",
    "    def convert_to_numpy(self, dataset):\n",
    "        audio_data = []\n",
    "        labels = []\n",
    "    \n",
    "        dataset = dataset.unbatch()\n",
    "        \n",
    "        for audio, label in dataset:\n",
    "            audio_data.append(audio.numpy())  # Assuming audio is a tensor, convert to numpy array\n",
    "            labels.append(label.numpy())      # Assuming label is a tensor, convert to numpy array\n",
    "        \n",
    "        audio_data = np.array(audio_data)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        return audio_data, labels\n",
    "    \n",
    "    # INIZIO FILTERBANKS\n",
    "    def makeHamming(self, M):\n",
    "        R = (( M - 1 ) / 2 , M / 2)[M % 2 == 0]\n",
    "        w = (np.hamming(M), np.hamming(M + 1))[M % 2 == 0]\n",
    "        if M % 2 != 0:\n",
    "            w[0] = w[0]/2\n",
    "            w[M-1] = w[M-1]/2\n",
    "        else:\n",
    "            w = w[:M]\n",
    "    \n",
    "        return w\n",
    "\n",
    "    def hztomel(self, hz):\n",
    "        return (2595 * np.log10(1 + hz / 700))\n",
    "\n",
    "    def meltohz(self, mel):\n",
    "        return (700 * (10**(mel / 2595) - 1))\n",
    "\n",
    "    def compute_filterbanks(self, audios_np, pre_emphasis=0.97, sample_rate=16000, frame_size=0.025, frame_stride=0.01, NFFT=512, nfilt=40):\n",
    "        filterbanks_np = []\n",
    "        \n",
    "        for samples in audios_np:\n",
    "            emphasized_audio = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "            audio_length = len(emphasized_audio)\n",
    "    \n",
    "            frame_length, frame_step = int(frame_size * sample_rate), int(frame_stride * sample_rate)\n",
    "    \n",
    "            num_frames = int(np.ceil(float(np.abs(audio_length - frame_length)) / frame_step))\n",
    "    \n",
    "            pad_audio_length = num_frames * frame_step + frame_length\n",
    "            z = np.zeros((pad_audio_length - audio_length))\n",
    "            pad_audio = np.append(emphasized_audio, z)\n",
    "    \n",
    "            indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "            frames = pad_audio[indices.astype(np.int32, copy=False)]\n",
    "    \n",
    "            # Usiamo la funzione di Hamming\n",
    "            hamming_window = self.makeHamming(frame_length)\n",
    "    \n",
    "            mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitudo della FFT\n",
    "            pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))\n",
    "    \n",
    "            # convertiamo hz in mel\n",
    "            low_freq_mel = self.hztomel(0)\n",
    "            high_freq_mel = self.hztomel(sample_rate / 2)\n",
    "    \n",
    "            mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "            hz_points = self.meltohz(mel_points) \n",
    "    \n",
    "            bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "    \n",
    "            fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    \n",
    "            for m in range(1, nfilt + 1):\n",
    "                f_m_minus = int(bin[m - 1])\n",
    "                f_m = int(bin[m])\n",
    "                f_m_plus = int(bin[m + 1])\n",
    "    \n",
    "                for k in range(f_m_minus, f_m):\n",
    "                    fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "                for k in range(f_m, f_m_plus):\n",
    "                    fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    \n",
    "            # in questo momento invece calcoliamo i filter banks per i segmenti di audio, utilizzando i filtri triangolari appena creati\n",
    "            filter_banks = np.dot(pow_frames, fbank.T)\n",
    "            filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "            filter_banks = 20 * np.log10(filter_banks)\n",
    "    \n",
    "            filterbanks_np.append(filter_banks)\n",
    "        \n",
    "        return np.array(filterbanks_np)\n",
    "    \n",
    "    def get_filterbanks_dataset(self): \n",
    "        audios, labels = self.convert_to_numpy(self.dataset)\n",
    "\n",
    "        filterbanks = self.compute_filterbanks(audios)\n",
    "        filterbanks = np.expand_dims(filterbanks, axis=-1)\n",
    "\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((filterbanks, labels))\n",
    "        self.dataset = self.dataset.batch(32)\n",
    "        \n",
    "        return self.dataset\n",
    "    # FINE FILTERBANKS\n",
    "\n",
    "    # INIZIO MFCC\n",
    "    def compute_mfcc(self, filter_banks, num_ceps=12, cep_lifter=22):\n",
    "        mfcc_np = []\n",
    "        \n",
    "        for f in filter_banks:\n",
    "            mfcc = scipy.dct(f, type=2, axis=1, norm='ortho')[:, 1 : (num_ceps + 1)]\n",
    "\n",
    "            (nframes, ncoeff) = mfcc.shape\n",
    "            n = np.arange(ncoeff)\n",
    "            \n",
    "            lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)\n",
    "            mfcc *= lift\n",
    "\n",
    "            mfcc_np.append(mfcc)\n",
    "        \n",
    "        return np.array(mfcc_np)\n",
    "    \n",
    "    \n",
    "    def get_mfcc_dataset(self):\n",
    "        audios, labels = self.convert_to_numpy(self.dataset)\n",
    "\n",
    "        filterbanks = self.compute_filterbanks(audios)\n",
    "        mfcc = self.compute_mfcc(filterbanks)\n",
    "        \n",
    "        mfcc = np.expand_dims(mfcc, axis=-1)\n",
    "        \n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((mfcc, labels))\n",
    "        self.dataset = self.dataset.batch(32)\n",
    "        \n",
    "        return self.dataset\n",
    "    \n",
    "    # FINE MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e9da6-e58e-4ab9-96f4-4a469f4af70d",
   "metadata": {},
   "source": [
    "## Importazione funzioni ulteriori\n",
    "Oltre alla classe per la conversione del dataset ci torneranno sicuramente utili le seguenti funzioni: \n",
    "- `convert_history_to_csv`, per salvare l'history dell'addestramento in un csv\n",
    "- `convert_model_to_onnx`, per convertire il miglior modello **.keras** in **.onnx**\n",
    "- `create_train_val_plot`, per visualizzare l'andamento dell'accuratezza e della perdita durante l'addestramento\n",
    "- `evaluate_onnx_model`, per valutare il modello sui dati di test con il modello **.onnx**\n",
    "- `create_distribution_plot`, per visualizzare la distribuzione di accuratezza delle classi su un'immagine casuale\n",
    "\n",
    "Tutte queste funzioni le abbiamo già viste, perciò non ci soffermeremo troppo su di esse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb3e5579-fd34-469a-93b3-8793c6f21504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_history_to_csv(model_history):\n",
    "    # converto la history del dataframe in un dataframe pandas\n",
    "    model_history_df = pd.DataFrame(model_history.history)\n",
    "    # cambio il nome dell'indice e lo imposto a partire da 1\n",
    "    model_history_df.index = range(1, len(model_history_df) + 1)\n",
    "    model_history_df.index.name = \"epochs\"\n",
    "\n",
    "    return model_history_df\n",
    "\n",
    "def convert_model_to_onnx(model_path, input_shape):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    onnx_path = model_path.split(\".\")[:-1] + [\"onnx\"]\n",
    "    onnx_path = \".\".join(onnx_path)\n",
    "    \n",
    "    input_signature = [tf.TensorSpec((None, *input_shape), tf.float32, name=\"input\")]\n",
    "    onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=input_signature)\n",
    "\n",
    "    onnx.save(onnx_model, onnx_path)\n",
    "\n",
    "def create_train_val_plot(history, overfit=True):\n",
    "    # definisco numero epoche\n",
    "    epochs = range(1, len(history['accuracy']) + 1)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = history['accuracy']\n",
    "    val_accuracy = history['val_accuracy']\n",
    "\n",
    "    fig_acc, ax_acc = plt.subplots()\n",
    "\n",
    "    # linea di base\n",
    "    ax_acc.plot(epochs, [0.5 for x in range(len(epochs))], color=\"lightgray\", linestyle=\"--\", label=\"Base\")\n",
    "    # linee di accuratezza\n",
    "    ax_acc.plot(epochs, accuracy, color=\"slategray\", label=\"Accuratezza in addestramento\")\n",
    "    ax_acc.plot(epochs, val_accuracy, color=\"indianred\", label=\"Accuratezza in validazione\")\n",
    "    # miglior accuratezza validazione\n",
    "    ax_acc.axhline(y=max(val_accuracy), c='indianred', alpha=0.7, linestyle='--')\n",
    "    ax_acc.text(len(epochs) * 1.07, max(val_accuracy), round(max(val_accuracy), 2), fontsize=9, fontweight=\"bold\", horizontalalignment=\"left\", verticalalignment=\"center\")\n",
    "    # nascondo limiti plot\n",
    "    ax_acc.spines[\"right\"].set_visible(False)\n",
    "    ax_acc.spines[\"top\"].set_visible(False)\n",
    "    ax_acc.spines[\"left\"].set_visible(False)\n",
    "    # definizione label e titolo + legenda\n",
    "    ax_acc.set_title(\"Accuratezza in addestramento e validazione\")\n",
    "    ax_acc.set_xlabel(\"Epoche\")\n",
    "    ax_acc.set_ylabel(\"Accuratezza (%)\")\n",
    "    ax_acc.legend()\n",
    "\n",
    "    # Loss\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    fig_loss, ax_loss = plt.subplots()\n",
    "\n",
    "    # linee di loss\n",
    "    ax_loss.plot(epochs, loss, color=\"slategray\", label=\"Perdita in addestramento\")\n",
    "    ax_loss.plot(epochs, val_loss, color=\"indianred\", label=\"Perdita in validazione\")\n",
    "    # miglior loss validazione\n",
    "    ax_loss.axhline(y=min(val_loss), c='indianred', alpha=0.7, linestyle='--')\n",
    "    ax_loss.text(len(epochs) * 1.07, min(val_loss), round(min(val_loss), 2), fontsize=9, fontweight=\"bold\", horizontalalignment=\"left\", verticalalignment=\"center\")\n",
    "    # nascondo limiti plot\n",
    "    ax_loss.spines[\"right\"].set_visible(False)\n",
    "    ax_loss.spines[\"top\"].set_visible(False)\n",
    "    ax_loss.spines[\"left\"].set_visible(False)\n",
    "    # definizione label e titolo + legenda\n",
    "    ax_loss.set_title(\"Perdita in addestramento e validazione\")\n",
    "    ax_loss.set_xlabel(\"Epoche\")\n",
    "    ax_loss.set_ylabel(\"Perdita\")\n",
    "\n",
    "    handles, _ = ax_loss.get_legend_handles_labels()\n",
    "\n",
    "    if overfit:\n",
    "        # area overfit\n",
    "        rect = mpatches.Rectangle((np.argmin(val_loss) + 1, 0), width=100 - np.argmin(val_loss), height=max(max(loss), max(val_loss)), color='lightcoral', alpha=0.3)\n",
    "        ax_loss.add_patch(rect)\n",
    "        # patch overfit per legenda\n",
    "        overfit = mpatches.Patch(color='indianred', alpha=0.3, label='Area overfit')\n",
    "        handles, _ = ax_loss.get_legend_handles_labels()\n",
    "        handles.append(overfit)\n",
    "\n",
    "    # legenda\n",
    "    ax_loss.legend(handles=handles)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_onnx_model(path_model_onnx, test_ds):\n",
    "    # il fatto che sia suddiviso in batch mi crea problemi, perciò lo risolvo togliendoli\n",
    "    test_ds = test_ds.unbatch()\n",
    "    \n",
    "    # carico il modello utilizzando il file onnx\n",
    "    m = rt.InferenceSession(path_model_onnx)\n",
    "    \n",
    "    # trasformo il dataset in array numpy\n",
    "    spectrogram_np = np.array([spectrogram.numpy() for spectrogram, _ in test_ds])\n",
    "    labels_np = np.array([label.numpy() for _, label in test_ds])\n",
    "    \n",
    "    # eseguo le predizione del modello\n",
    "    pred_onnx = m.run(None, {'input': spectrogram_np})\n",
    "    # ottengo la predizione corretta\n",
    "    predictions = np.argmax(pred_onnx[0], axis=1)\n",
    "    # computo la accuratezza\n",
    "    accuracy = np.mean(predictions == labels_np)\n",
    "    # stampo l'accuratezza\n",
    "    print(f\"Accuratezza: {accuracy:.3f}\")\n",
    "\n",
    "def create_distribution_plot(model, audio, label):\n",
    "    prediction = model(audio) # otteniamo la predizione del modello sull'audio preso in input\n",
    "\n",
    "    random_audio_index = np.random.randint(0, len(prediction) - 1) # abbiamo notato che mostravamo sempre la classe no, magari così cambia un po' le classi che mostriamo\n",
    "\n",
    "    softmax_pred = tf.nn.softmax(prediction[random_audio_index]) # dobbiamo applicare nuovamente la softmax in modo tale da avere dei dati leggibili\n",
    "    pred_label = np.argmax(softmax_pred) # ci salviamo l'indice della label predetta dal nostro modello, quella con la probabilità più alta\n",
    "\n",
    "    colors = ['indianred'] * len(label_names) # creiamo un vettore di lunghezza 30 (la quantità delle nostre label) colorate di rosso\n",
    "\n",
    "    if pred_label == label[random_audio_index].numpy(): # se la label che abbiamo predetto è corretta, allora la coloriamo di verde\n",
    "        colors[pred_label] = 'seagreen'\n",
    "    else: # se la label che abbiamo predetto è sbagliata, allora coloriamo di verde la corretta e di gialla la predetta\n",
    "        colors[pred_label] = 'khaki'\n",
    "        colors[label[random_audio_index].numpy()] = 'seagreen'\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.bar(label_names, softmax_pred.numpy(), color=colors)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # dichiariamo i colori della legenda\n",
    "    other_values_patch = mpatches.Patch(color='indianred', label='Altre classi')\n",
    "    pred_value_patch = mpatches.Patch(color='khaki', label='Classe predetta')\n",
    "    correct_value_patch = mpatches.Patch(color='green', label='Classe corretta')\n",
    "\n",
    "    # disegniamo la legenda con i colori necessari\n",
    "    if pred_label == label[random_audio_index].numpy(): # se abbiamo predetto correttamente la label, non è necessario mostrare il colore giallo (non è presente nel plot)\n",
    "        plt.legend(handles=[other_values_patch, correct_value_patch])\n",
    "    else:\n",
    "        plt.legend(handles=[other_values_patch, pred_value_patch, correct_value_patch])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376eacc3-d029-463c-90b4-d2e00e4a3865",
   "metadata": {},
   "source": [
    "Carichiamo il dataset di addestramento e validazione/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ea60450-56b6-4ef1-9ac3-b8e4f7339bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12933 files belonging to 30 classes.\n",
      "Using 7760 files for training.\n",
      "Using 5173 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds, validation_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory='../reduced_dataset/dataset/audio',\n",
    "    validation_split=0.4, # stiamo mettendo da parte il 40% del dataset, che sarà suddiviso in validation set e test set\n",
    "    shuffle=True,\n",
    "    subset='both', # necessario se stiamo utilizzando validation_split (se no darebbe errore)\n",
    "    seed=0 # necessario se stiamo utilizzando sia shuffle che validation_split (se no darebbe errore)\n",
    ")\n",
    "\n",
    "label_names = train_ds.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ed7e06-fdab-4f2f-9fdc-2d65f028b242",
   "metadata": {},
   "source": [
    "## Addestramento modello convolutivo base (filterbanks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6e5e5-bede-46c0-b557-f43285f008d5",
   "metadata": {},
   "source": [
    "Per la conversione del dataset è necessario richiamare la classe dandogli in input il dataset, e dopodiché chiamare il metodo \"convert\" con in input l'opzione scelta tra: spettrogrammi, filterbanks, mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a41c20-0ff2-4e42-93d2-9189ce7f41cf",
   "metadata": {},
   "source": [
    "L'addestramento convolutivo degli spettrogrammi è già stato effettuato, perciò adesso procederemo con l'addestramento del modello con i filterbanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e7c7002-b5fe-40be-ac8b-9d734cea7002",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fb_train_ds \u001b[38;5;241m=\u001b[39m DatasetConverter(train_ds)\n\u001b[1;32m----> 2\u001b[0m fb_train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mfb_train_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilterbanks\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m fb_validation_ds \u001b[38;5;241m=\u001b[39m DatasetConverter(validation_ds)\n\u001b[0;32m      5\u001b[0m fb_validation_ds \u001b[38;5;241m=\u001b[39m fb_validation_ds\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilterbanks\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m, in \u001b[0;36mDatasetConverter.convert\u001b[1;34m(self, option)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_spectrogram_dataset()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m option \u001b[38;5;241m==\u001b[39m available_options[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_filterbanks_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m option \u001b[38;5;241m==\u001b[39m available_options[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mfcc_dataset()\n",
      "Cell \u001b[1;32mIn[2], line 128\u001b[0m, in \u001b[0;36mDatasetConverter.get_filterbanks_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_filterbanks_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m): \n\u001b[0;32m    126\u001b[0m     audios, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_to_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m--> 128\u001b[0m     filterbanks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_filterbanks\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudios\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     filterbanks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(filterbanks, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((filterbanks, labels))\n",
      "Cell \u001b[1;32mIn[2], line 117\u001b[0m, in \u001b[0;36mDatasetConverter.compute_filterbanks\u001b[1;34m(self, audios_np, pre_emphasis, sample_rate, frame_size, frame_stride, NFFT, nfilt)\u001b[0m\n\u001b[0;32m    114\u001b[0m         fbank[m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, k] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mbin\u001b[39m[m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m k) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mbin\u001b[39m[m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mbin\u001b[39m[m])\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# in questo momento invece calcoliamo i filter banks per i segmenti di audio, utilizzando i filtri triangolari appena creati\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m filter_banks \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpow_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfbank\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m filter_banks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(filter_banks \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps, filter_banks)\n\u001b[0;32m    119\u001b[0m filter_banks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog10(filter_banks)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fb_train_ds = DatasetConverter(train_ds)\n",
    "fb_train_ds = fb_train_ds.convert('filterbanks')\n",
    "\n",
    "fb_validation_ds = DatasetConverter(validation_ds)\n",
    "fb_validation_ds = fb_validation_ds.convert('filterbanks')\n",
    "\n",
    "fb_val_ds = fb_validation_ds.take(fb_validation_ds.cardinality() // 2)\n",
    "fb_test_ds = fb_validation_ds.skip(fb_validation_ds.cardinality() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94013fc2-13c5-474b-88d7-8d0aba5556b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_model(input_shape):\n",
    "    # Dichiariamo lo shape dell'input, ridimensioniamo le immagini, e normalizziamo\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"inputs\")\n",
    "    x = tf.keras.layers.Normalization(name=\"normalizzazione\")(inputs)\n",
    "\n",
    "    # Blocco di apprendimento delle caratteristiche\n",
    "    conv2D_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_1\")(x)\n",
    "    MaxPooling2D_1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_1\")(conv2D_1)\n",
    "    conv2D_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_2\")(MaxPooling2D_1)\n",
    "    MaxPooling2D_2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_2\")(conv2D_2)\n",
    "    conv2D_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_3\")(MaxPooling2D_2)\n",
    "    MaxPooling2D_3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_3\")(conv2D_3)\n",
    "    conv2D_4 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_4\")(MaxPooling2D_3)\n",
    "    MaxPooling2D_4 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_4\", padding=\"same\")(conv2D_4)\n",
    "    conv2D_5 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_5\")(MaxPooling2D_4)\n",
    "\n",
    "    # Blocco di classificazione\n",
    "    Flatten = tf.keras.layers.Flatten(name=\"Flatten\")(conv2D_5)\n",
    "    outputs = tf.keras.layers.Dense(30, activation=\"softmax\", name=\"dense_output\")(Flatten)\n",
    "\n",
    "    # Modello\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27d0ba93-9f29-415b-94f6-9ce45bea4a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 98, 12, 1)]       0         \n",
      "                                                                 \n",
      " normalizzazione (Normalizat  (None, 98, 12, 1)        3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv2D_1 (Conv2D)           (None, 98, 12, 32)        320       \n",
      "                                                                 \n",
      " MaxPooling2D_1 (MaxPooling2  (None, 49, 6, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2D_2 (Conv2D)           (None, 49, 6, 64)         18496     \n",
      "                                                                 \n",
      " MaxPooling2D_2 (MaxPooling2  (None, 24, 3, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2D_3 (Conv2D)           (None, 24, 3, 128)        73856     \n",
      "                                                                 \n",
      " MaxPooling2D_3 (MaxPooling2  (None, 12, 1, 128)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2D_4 (Conv2D)           (None, 12, 1, 256)        295168    \n",
      "                                                                 \n",
      " MaxPooling2D_4 (MaxPooling2  (None, 6, 1, 256)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2D_5 (Conv2D)           (None, 6, 1, 256)         590080    \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 1536)              0         \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 30)                46110     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,024,033\n",
      "Trainable params: 1,024,030\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for audio, label in train_ds:\n",
    "    basic_model = get_basic_model(audio.shape[1:])\n",
    "    break\n",
    "    \n",
    "basic_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "\n",
    "basic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12cbf6a8-0bb4-49aa-9de6-4c1b9022f30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "243/243 [==============================] - 4s 5ms/step - loss: 2.9011 - accuracy: 0.2323 - val_loss: 1.7944 - val_accuracy: 0.4838\n",
      "Epoch 2/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 1.2618 - accuracy: 0.6291 - val_loss: 1.0200 - val_accuracy: 0.7118\n",
      "Epoch 3/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.7507 - accuracy: 0.7800 - val_loss: 0.8895 - val_accuracy: 0.7566\n",
      "Epoch 4/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.4972 - accuracy: 0.8485 - val_loss: 0.9279 - val_accuracy: 0.7762\n",
      "Epoch 5/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.3678 - accuracy: 0.8866 - val_loss: 1.1592 - val_accuracy: 0.7832\n",
      "Epoch 6/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.3164 - accuracy: 0.9030 - val_loss: 1.2137 - val_accuracy: 0.7770\n",
      "Epoch 7/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2619 - accuracy: 0.9226 - val_loss: 1.2777 - val_accuracy: 0.8002\n",
      "Epoch 8/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2525 - accuracy: 0.9323 - val_loss: 1.0395 - val_accuracy: 0.8268\n",
      "Epoch 9/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2327 - accuracy: 0.9339 - val_loss: 1.4344 - val_accuracy: 0.8106\n",
      "Epoch 10/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2148 - accuracy: 0.9429 - val_loss: 1.5254 - val_accuracy: 0.8121\n",
      "Epoch 11/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2366 - accuracy: 0.9434 - val_loss: 1.8277 - val_accuracy: 0.7851\n",
      "Epoch 12/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2309 - accuracy: 0.9451 - val_loss: 1.6815 - val_accuracy: 0.8067\n",
      "Epoch 13/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2064 - accuracy: 0.9545 - val_loss: 2.2123 - val_accuracy: 0.7940\n",
      "Epoch 14/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2164 - accuracy: 0.9523 - val_loss: 1.8322 - val_accuracy: 0.8252\n",
      "Epoch 15/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2004 - accuracy: 0.9576 - val_loss: 2.0513 - val_accuracy: 0.8349\n",
      "Epoch 16/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2198 - accuracy: 0.9595 - val_loss: 2.3320 - val_accuracy: 0.8187\n",
      "Epoch 17/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.2010 - accuracy: 0.9607 - val_loss: 2.3598 - val_accuracy: 0.8036\n",
      "Epoch 18/30\n",
      "243/243 [==============================] - 1s 4ms/step - loss: 0.1827 - accuracy: 0.9647 - val_loss: 2.5424 - val_accuracy: 0.8144\n",
      "Epoch 19/30\n",
      "138/243 [================>.............] - ETA: 0s - loss: 0.2001 - accuracy: 0.9644"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m basic_model_callbacks \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbestmodels/rmsprop/basic_model_fb.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m----> 2\u001b[0m basic_model_history \u001b[38;5;241m=\u001b[39m \u001b[43mbasic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbasic_model_callbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "basic_model_callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=\"bestmodels/rmsprop/basic_model_fb.keras\", save_best_only=True, monitor=\"val_loss\")]\n",
    "basic_model_history = basic_model.fit(train_ds, epochs=30, batch_size=32, validation_data=val_ds, callbacks=basic_model_callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
