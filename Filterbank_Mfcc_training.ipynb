{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480c543a-b196-43c1-9ae0-b2a1b15ff310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.fftpack as scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a424b5d-76ec-4f49-937e-c3ef68a405b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConverter:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def convert(self, option):\n",
    "        available_options = ['spettrogrammi', 'filterbanks', 'mfcc']\n",
    "        \n",
    "        if option == available_options[0]:\n",
    "            return self.get_spectrogram_dataset()\n",
    "        elif option == available_options[1]:\n",
    "            return self.get_filterbanks_dataset()\n",
    "        elif option == available_options[2]:\n",
    "            return self.get_mfcc_dataset()\n",
    "        else:\n",
    "            raise ValueError(f\"Opzione non disponibile: inserire una delle seguenti opzioni: {available_options}\")\n",
    "    \n",
    "    # INIZIO SPETTROGRAMMI\n",
    "    def squeeze(self, audio, labels):\n",
    "        audio = tf.squeeze(audio, axis=-1)\n",
    "        return audio, labels\n",
    "    \n",
    "    def get_spectrogram(self, waveform):\n",
    "    # applichiamo la short-time Fourier transorm\n",
    "        spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n",
    "        spectrogram = tf.abs(spectrogram)\n",
    "        \n",
    "        return spectrogram[..., tf.newaxis]\n",
    "    \n",
    "    def get_spectrogram_dataset(self):\n",
    "        # squeeze\n",
    "        self.dataset = self.dataset.map(self.squeeze, tf.data.AUTOTUNE)\n",
    "        self.dataset = self.dataset.map(lambda x, y: (self.get_spectrogram(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return self.dataset\n",
    "\n",
    "    # FINE SPETTROGRAMMI\n",
    "\n",
    "    def convert_to_numpy(self, dataset):\n",
    "        audio_data = []\n",
    "        labels = []\n",
    "    \n",
    "        dataset = dataset.unbatch()\n",
    "        \n",
    "        for audio, label in dataset:\n",
    "            audio_data.append(audio.numpy())  # Assuming audio is a tensor, convert to numpy array\n",
    "            labels.append(label.numpy())      # Assuming label is a tensor, convert to numpy array\n",
    "        \n",
    "        audio_data = np.array(audio_data)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        return audio_data, labels\n",
    "    \n",
    "    # INIZIO FILTERBANKS\n",
    "    def makeHamming(self, M):\n",
    "        R = (( M - 1 ) / 2 , M / 2)[M % 2 == 0]\n",
    "        w = (np.hamming(M), np.hamming(M + 1))[M % 2 == 0]\n",
    "        if M % 2 != 0:\n",
    "            w[0] = w[0]/2\n",
    "            w[M-1] = w[M-1]/2\n",
    "        else:\n",
    "            w = w[:M]\n",
    "    \n",
    "        return w\n",
    "\n",
    "    def hztomel(self, hz):\n",
    "        return (2595 * np.log10(1 + hz / 700))\n",
    "\n",
    "    def meltohz(self, mel):\n",
    "        return (700 * (10**(mel / 2595) - 1))\n",
    "\n",
    "    def compute_filterbanks(self, audios_np, pre_emphasis=0.97, sample_rate=16000, frame_size=0.025, frame_stride=0.01, NFFT=512, nfilt=40):\n",
    "        filterbanks_np = []\n",
    "        \n",
    "        for samples in audios_np:\n",
    "            emphasized_audio = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "            audio_length = len(emphasized_audio)\n",
    "    \n",
    "            frame_length, frame_step = int(frame_size * sample_rate), int(frame_stride * sample_rate)\n",
    "    \n",
    "            num_frames = int(np.ceil(float(np.abs(audio_length - frame_length)) / frame_step))\n",
    "    \n",
    "            pad_audio_length = num_frames * frame_step + frame_length\n",
    "            z = np.zeros((pad_audio_length - audio_length))\n",
    "            pad_audio = np.append(emphasized_audio, z)\n",
    "    \n",
    "            indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "            frames = pad_audio[indices.astype(np.int32, copy=False)]\n",
    "    \n",
    "            # Usiamo la funzione di Hamming\n",
    "            hamming_window = self.makeHamming(frame_length)\n",
    "    \n",
    "            mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitudo della FFT\n",
    "            pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))\n",
    "    \n",
    "            # convertiamo hz in mel\n",
    "            low_freq_mel = self.hztomel(0)\n",
    "            high_freq_mel = self.hztomel(sample_rate / 2)\n",
    "    \n",
    "            mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "            hz_points = self.meltohz(mel_points) \n",
    "    \n",
    "            bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "    \n",
    "            fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    \n",
    "            for m in range(1, nfilt + 1):\n",
    "                f_m_minus = int(bin[m - 1])\n",
    "                f_m = int(bin[m])\n",
    "                f_m_plus = int(bin[m + 1])\n",
    "    \n",
    "                for k in range(f_m_minus, f_m):\n",
    "                    fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "                for k in range(f_m, f_m_plus):\n",
    "                    fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    \n",
    "            # in questo momento invece calcoliamo i filter banks per i segmenti di audio, utilizzando i filtri triangolari appena creati\n",
    "            filter_banks = np.dot(pow_frames, fbank.T)\n",
    "            filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "            filter_banks = 20 * np.log10(filter_banks)\n",
    "    \n",
    "            filterbanks_np.append(filter_banks)\n",
    "        \n",
    "        return np.array(filterbanks_np)\n",
    "    \n",
    "    def get_filterbanks_dataset(self): \n",
    "        audios, labels = self.convert_to_numpy(self.dataset)\n",
    "\n",
    "        filterbanks = self.compute_filterbanks(audios)\n",
    "        filterbanks = np.expand_dims(filterbanks, axis=-1)\n",
    "\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((filterbanks, labels))\n",
    "        self.dataset = self.dataset.batch(32)\n",
    "        \n",
    "        return self.dataset\n",
    "    # FINE FILTERBANKS\n",
    "\n",
    "    # FINE MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea60450-56b6-4ef1-9ac3-b8e4f7339bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12933 files belonging to 30 classes.\n",
      "Using 7760 files for training.\n",
      "Using 5173 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds, validation_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory='../reduced_dataset/dataset/audio',\n",
    "    validation_split=0.4, # stiamo mettendo da parte il 40% del dataset, che sar√† suddiviso in validation set e test set\n",
    "    shuffle=True,\n",
    "    subset='both', # necessario se stiamo utilizzando validation_split (se no darebbe errore)\n",
    "    seed=0 # necessario se stiamo utilizzando sia shuffle che validation_split (se no darebbe errore)\n",
    ")\n",
    "\n",
    "label_names = train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e7c7002-b5fe-40be-ac8b-9d734cea7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DatasetConverter(train_ds)\n",
    "train_ds = train_ds.convert('filterbanks')\n",
    "\n",
    "validation_ds = DatasetConverter(validation_ds)\n",
    "validation_ds = validation_ds.convert('filterbanks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06924904-7ffc-44fe-a56e-1755c9069c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = validation_ds.take(validation_ds.cardinality() // 2)\n",
    "test_ds = validation_ds.skip(validation_ds.cardinality() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94013fc2-13c5-474b-88d7-8d0aba5556b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_model(input_shape):\n",
    "    # Dichiariamo lo shape dell'input, ridimensioniamo le immagini, e normalizziamo\n",
    "    inputs = tf.keras.Input(shape=input_shape, name=\"inputs\")\n",
    "    x = tf.keras.layers.Normalization(name=\"normalizzazione\")(inputs)\n",
    "\n",
    "    # Blocco di apprendimento delle caratteristiche\n",
    "    conv2D_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_1\")(x)\n",
    "    MaxPooling2D_1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_1\")(conv2D_1)\n",
    "    conv2D_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_2\")(MaxPooling2D_1)\n",
    "    MaxPooling2D_2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_2\")(conv2D_2)\n",
    "    conv2D_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_3\")(MaxPooling2D_2)\n",
    "    MaxPooling2D_3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_3\")(conv2D_3)\n",
    "    conv2D_4 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_4\")(MaxPooling2D_3)\n",
    "    MaxPooling2D_4 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_4\")(conv2D_4)\n",
    "    conv2D_5 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_5\")(MaxPooling2D_4)\n",
    "\n",
    "    # Blocco di classificazione\n",
    "    Flatten = tf.keras.layers.Flatten(name=\"Flatten\")(conv2D_5)\n",
    "    outputs = tf.keras.layers.Dense(30, activation=\"softmax\", name=\"dense_output\")(Flatten)\n",
    "\n",
    "    # Modello\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d0ba93-9f29-415b-94f6-9ce45bea4a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 98, 40, 1)]       0         \n",
      "                                                                 \n",
      " normalizzazione (Normalizat  (None, 98, 40, 1)        3         \n",
      " ion)                                                            \n",
      "                                                                 \n",
      " conv2D_1 (Conv2D)           (None, 98, 40, 32)        320       \n",
      "                                                                 \n",
      " MaxPooling2D_1 (MaxPooling2  (None, 49, 20, 32)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2D_2 (Conv2D)           (None, 49, 20, 64)        18496     \n",
      "                                                                 \n",
      " MaxPooling2D_2 (MaxPooling2  (None, 24, 10, 64)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2D_3 (Conv2D)           (None, 24, 10, 128)       73856     \n",
      "                                                                 \n",
      " MaxPooling2D_3 (MaxPooling2  (None, 12, 5, 128)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2D_4 (Conv2D)           (None, 12, 5, 256)        295168    \n",
      "                                                                 \n",
      " MaxPooling2D_4 (MaxPooling2  (None, 6, 2, 256)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2D_5 (Conv2D)           (None, 6, 2, 256)         590080    \n",
      "                                                                 \n",
      " Flatten (Flatten)           (None, 3072)              0         \n",
      "                                                                 \n",
      " dense_output (Dense)        (None, 30)                92190     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,070,113\n",
      "Trainable params: 1,070,110\n",
      "Non-trainable params: 3\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for audio, label in train_ds:\n",
    "    basic_model = get_basic_model(audio.shape[1:])\n",
    "    break\n",
    "    \n",
    "basic_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "\n",
    "basic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12cbf6a8-0bb4-49aa-9de6-4c1b9022f30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "243/243 [==============================] - 6s 6ms/step - loss: 3.2895 - accuracy: 0.1383 - val_loss: 1.9099 - val_accuracy: 0.4441\n",
      "Epoch 2/30\n",
      "243/243 [==============================] - 1s 6ms/step - loss: 1.3121 - accuracy: 0.6227 - val_loss: 0.9606 - val_accuracy: 0.7458\n",
      "Epoch 3/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.6148 - accuracy: 0.8144 - val_loss: 0.7371 - val_accuracy: 0.7921\n",
      "Epoch 4/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.3835 - accuracy: 0.8857 - val_loss: 0.5441 - val_accuracy: 0.8681\n",
      "Epoch 5/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.9169 - val_loss: 0.8360 - val_accuracy: 0.8252\n",
      "Epoch 6/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.2147 - accuracy: 0.9365 - val_loss: 1.2029 - val_accuracy: 0.7924\n",
      "Epoch 7/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1984 - accuracy: 0.9416 - val_loss: 0.7761 - val_accuracy: 0.8692\n",
      "Epoch 8/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1683 - accuracy: 0.9514 - val_loss: 0.7517 - val_accuracy: 0.8843\n",
      "Epoch 9/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1598 - accuracy: 0.9564 - val_loss: 1.0255 - val_accuracy: 0.8638\n",
      "Epoch 10/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1537 - accuracy: 0.9604 - val_loss: 1.1585 - val_accuracy: 0.8492\n",
      "Epoch 11/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1487 - accuracy: 0.9635 - val_loss: 0.9509 - val_accuracy: 0.8773\n",
      "Epoch 12/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1465 - accuracy: 0.9639 - val_loss: 0.8779 - val_accuracy: 0.8866\n",
      "Epoch 13/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1319 - accuracy: 0.9680 - val_loss: 1.5603 - val_accuracy: 0.8592\n",
      "Epoch 14/30\n",
      "243/243 [==============================] - 1s 6ms/step - loss: 0.1365 - accuracy: 0.9686 - val_loss: 1.0796 - val_accuracy: 0.8792\n",
      "Epoch 15/30\n",
      "243/243 [==============================] - 1s 6ms/step - loss: 0.1425 - accuracy: 0.9711 - val_loss: 1.3301 - val_accuracy: 0.8619\n",
      "Epoch 16/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1317 - accuracy: 0.9733 - val_loss: 1.4099 - val_accuracy: 0.8754\n",
      "Epoch 17/30\n",
      "243/243 [==============================] - 1s 6ms/step - loss: 0.1283 - accuracy: 0.9724 - val_loss: 1.1901 - val_accuracy: 0.8947\n",
      "Epoch 18/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1446 - accuracy: 0.9714 - val_loss: 1.1559 - val_accuracy: 0.8800\n",
      "Epoch 19/30\n",
      "243/243 [==============================] - 1s 6ms/step - loss: 0.1139 - accuracy: 0.9754 - val_loss: 1.5220 - val_accuracy: 0.8850\n",
      "Epoch 20/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1243 - accuracy: 0.9745 - val_loss: 1.2048 - val_accuracy: 0.8870\n",
      "Epoch 21/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1211 - accuracy: 0.9767 - val_loss: 1.6303 - val_accuracy: 0.8870\n",
      "Epoch 22/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1124 - accuracy: 0.9807 - val_loss: 1.6109 - val_accuracy: 0.8954\n",
      "Epoch 23/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1291 - accuracy: 0.9765 - val_loss: 1.6772 - val_accuracy: 0.8924\n",
      "Epoch 24/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1328 - accuracy: 0.9773 - val_loss: 1.9551 - val_accuracy: 0.8754\n",
      "Epoch 25/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1282 - accuracy: 0.9803 - val_loss: 1.6426 - val_accuracy: 0.8877\n",
      "Epoch 26/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1319 - accuracy: 0.9778 - val_loss: 2.0664 - val_accuracy: 0.8935\n",
      "Epoch 27/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1102 - accuracy: 0.9818 - val_loss: 2.8982 - val_accuracy: 0.8700\n",
      "Epoch 28/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1321 - accuracy: 0.9814 - val_loss: 2.1906 - val_accuracy: 0.8904\n",
      "Epoch 29/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1413 - accuracy: 0.9811 - val_loss: 1.9232 - val_accuracy: 0.8692\n",
      "Epoch 30/30\n",
      "243/243 [==============================] - 1s 5ms/step - loss: 0.1376 - accuracy: 0.9799 - val_loss: 2.5829 - val_accuracy: 0.8927\n"
     ]
    }
   ],
   "source": [
    "basic_model_callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=\"bestmodels/rmsprop/basic_model_fb.keras\", save_best_only=True, monitor=\"val_loss\")]\n",
    "basic_model_history = basic_model.fit(train_ds, epochs=30, batch_size=32, validation_data=val_ds, callbacks=basic_model_callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
