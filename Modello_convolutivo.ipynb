{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c7c54c-0a56-4540-88a7-861340b0fc4e",
   "metadata": {},
   "source": [
    "# Modello convolutivo di base\n",
    "In questo notebook costruiamo un **modello convolutivo di base** così da poter avere un'idea migliore del task che stiamo approcciando. Utilizzando un modello di base riusciamo a capire soprattutto che prestazioni riusciamo a ottenere senza troppi sforzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ad9ec-f968-4dee-b7ef-4aba3b74f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d649778a-9379-4ded-8188-65bd7639ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dbe38373-8cc2-4813-a7c9-6bcf77dc499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12933 files belonging to 30 classes.\n",
      "Using 7760 files for training.\n",
      "Using 5173 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train_ds, validation_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory='../reduced_dataset/dataset/audio', \n",
    "    validation_split=0.4, # stiamo mettendo da parte il 40% del dataset, che sarà suddiviso in validation set e test set\n",
    "    shuffle=True, \n",
    "    subset='both', # necessario se stiamo utilizzando validation_split (se no darebbe errore)\n",
    "    seed=0 # necessario se stiamo utilizzando sia shuffle che validation_split (se no darebbe errore)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7321ddc2-1252-4afa-9596-8fab00165855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma (shape) audio: (32, 16000, 1)\n",
      "Forma (shape) classi: (32,)\n"
     ]
    }
   ],
   "source": [
    "for audios, labels in train_ds:\n",
    "    print(\"Forma (shape) audio:\", audios.shape)\n",
    "    print(\"Forma (shape) classi:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d611f7d2-ee72-4cbe-956f-a440d96b4bf4",
   "metadata": {},
   "source": [
    "Possiamo notare che lo shape dell'audio corrisponde a (32, 16000, 1), questo vuol dire che ogni \"audio\" che stiamo stampando in realtà è un batch da 32 tensori di lunghezza 16000 unidimensionali. Ovviamente anche lo shape delle classi sarà 32 visto e considerata la presenza di 32 tensori per iterazione. Abbiamo perciò capito che il nostro dataset è suddiviso in batch da 32 di grandezza, questo perché di default la funzione `tf.keras.utils.audio_dataset_from_directory(directory)` ha il parametro `batch_size` impostato a 32 (non avendo specificato la grandezza del batch_size è stato quindi utilizzato automaticamente 32)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a19db-767d-4ae6-a2c9-6c2204f7f9d9",
   "metadata": {},
   "source": [
    "## Funzioni per processamento dei dati\n",
    "\n",
    "Il prossimo step è definire la funzione `squeeze(audio, labels)`, che come è possibile notare prende in input l'audio e le classi, e serve per applicare la funzione `tf.squeeze` ai file che scegliamo di passargli. Abbiamo utilizzato alcune volte la funzione `tf.squeeze` a lezione, ma non ci siamo mai soffermati su di essa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7b88665a-9de1-451b-9556-4753b4e90ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(audio, labels): \n",
    "    audio = tf.squeeze(audio, axis=-1) \n",
    "    return audio, labels \n",
    "\n",
    "# fonte: funzione presa dal seguente link: https://www.geeksforgeeks.org/audio-recognition-in-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32912fc-eedf-4871-a0e4-8cc06796659e",
   "metadata": {},
   "source": [
    "Questo passaggio di squeeze ci serve a rimuovere l'ultima asse dello shape dei nostri audio. Noi attualmente abbiamo uno shape di (32, 16000, 1), dove quel 1 finale è in realtà inutile. Quindi specificando in `tf.squeeze` il parametro **axis=-1** stiamo dicendo di rimuovere l'ultima dimensione, trasformando la forma del nostro audio in (32, 16000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02755025-9eeb-44f8-a51d-52e04d2e1059",
   "metadata": {},
   "source": [
    "Quella dimensione finale risulta attualmente inutile, quindi stiamo semplificando la forma del nostro audio e allo stesso tempo anche rendendo un minimo più efficiente il processamento dei dati stessi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a435c8d9-ca0a-4ff4-973a-c01856ba712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 16000)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE) \n",
    "validation_ds = validation_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "\n",
    "audio, label = next(iter(train_ds))\n",
    "print(audio.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53f77fa-a6b0-481f-bf33-22423e1cf891",
   "metadata": {},
   "source": [
    "Adesso che abbiamo semplificato la forma (shape) dei nostri audio possiamo procedere col trasformarli in spettrogrammi. Il nostro obbiettivo è utilizzare un modello convolutivo, che come sappiamo prende in input delle immagini. Noi attualmente abbiamo ancora dei tensori di audio, e non delle immagini, perciò dobbiamo convertire il nostro dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bd2d8a-7822-47d3-80a9-917af78ce093",
   "metadata": {},
   "source": [
    "## Conversione e suddivisione dataset\n",
    "Andremo ad utilizzare due funzioni: `get_spectrogram` e `get_spectrogram_dataset`. Queste due funzioni usate insieme ci ritornano un dataset di spettrogrammi. La prima funzione `get_spectrogram` converte il singolo audio in spettrogramma mentre la seconda funzione `get_spectrogram_dataset` richiama la prima funzione per ogni dato presente nel dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "60678f4c-ac40-4ffe-8b5c-d0a2b1fc8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(waveform): \n",
    "    spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128) \n",
    "    spectrogram = tf.abs(spectrogram) \n",
    "    \n",
    "    return spectrogram[..., tf.newaxis] \n",
    "\n",
    "# fonte: funzione presa dal seguente link: https://www.geeksforgeeks.org/audio-recognition-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "164aa4a3-b1ec-4bc6-9471-a2159ff787de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram_dataset(dataset):\n",
    "    dataset = dataset.map(lambda x, y: (get_spectrogram(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# fonte: funzione presa dal seguente link: https://www.geeksforgeeks.org/audio-recognition-in-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224328ae-01e8-4d25-9e7b-28d8d710984e",
   "metadata": {},
   "source": [
    "Definite le due funzioni possiamo procedere a ottenere il nostro training set, validation set, e test set di spettrogrammi. La suddivisione di dati ricordo essere rispettivamente 60%, 20%, 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dc45c0ce-4245-4a1e-abc5-10469e0b6e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_spectrogram_dataset(train_ds)\n",
    "validation_ds = get_spectrogram_dataset(validation_ds)\n",
    "\n",
    "val_ds = validation_ds.take(validation_ds.cardinality() // 2) # ho cambiato nome del validation_ds in modo tale da non creare problemi con l'istruzione seguente\n",
    "test_ds = validation_ds.skip(validation_ds.cardinality() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d9de604b-4464-4fea-878c-00dd586d89e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di audio training set: 7776\n",
      "Numero di audio validation set: 2592\n",
      "Numero di audio test set: 2592\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero di audio training set:\", len(train_ds) * 32)\n",
    "print(\"Numero di audio validation set:\", len(val_ds) * 32)\n",
    "print(\"Numero di audio test set:\", len(test_ds) * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a607b-91a2-4f2e-abec-361f20f24259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
