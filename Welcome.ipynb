{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb067261-4951-4a08-a838-421c28598368",
   "metadata": {},
   "source": [
    "# Configurazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e00e3b-0f45-45a0-bb92-3a06edc0cb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow numpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61ba5b-7aa7-4750-b9fe-be77ce4503c3",
   "metadata": {},
   "source": [
    "# Indice dei Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a16d7-c333-4b27-a31c-931fff3f0b55",
   "metadata": {},
   "source": [
    "Lo scopo di questo progetto è creare dei modelli di Reti Neurali che siano in grado di riconoscere comandi vocali. Per fare questo abbiamo scelto di utilizzare il dataset pubblico [TensorFlow Speech Recognition Challenge](https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/code?competitionId=7634&sortBy=voteCount&excludeNonAccessedDatasources=true) di Google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32630d2a-3133-4026-b50f-f4db9361cd85",
   "metadata": {},
   "source": [
    "## Analisi dei Dati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c4ad5d-861f-42ff-8c78-e35f97c37114",
   "metadata": {},
   "source": [
    "Il dataset disponibile era di dimensioni eccessive per il nostro progetto, quindi abbiamo deciso di iniziare [riducendo il dataset](Riduzione_Dataset.ipynb). \n",
    "\n",
    "Successivamente siamo passati una fase di [analisi esplorativa](Analisi_Esplorativa.ipynb). Questo ci ha permesso di capire quali tipi di dati fossero presenti nel dataset e iniziare a pensare delle possibili soluzioni alla challenge proposta. Fatto ciò, ci siamo posti il problema di rappresentare al meglio i dati a disposizione, cosi da poterli analizzare più profondamente. Le soluzioni trovate sono state la visualizzazione di:\n",
    "- [spettrogrammi](Spectrogram.ipynb)\n",
    "- [segnali degli audio](Wave_audio.ipynb) \n",
    "- [filter banks](Filter_Banks.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9c2e1-fe4c-4743-ac18-ab1c4f5c8deb",
   "metadata": {},
   "source": [
    "## Modelli\n",
    "\n",
    "Esplorati i dati ed eseguiti i pre-processamenti necessari, siamo passati alla creazione dei modelli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93176b6f-2541-41ce-992f-db07de95dd4d",
   "metadata": {},
   "source": [
    "### Convolutivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281e9c2-e4bc-4435-880a-8074293dadb9",
   "metadata": {},
   "source": [
    "Considerando la conversione degli audio in spettrogrammi durante le fasi precedenti, il primo su cui ci siamo concentrati è stato un [modello convolutivo base](Modello_convolutivo.ipynb), con ottimizzatore RMSProp. Abbiamo eseguito alcuni esperimenti prima di riuscire a trovare un'architettura in grado di eseguire il compito di classificazione dei comandi, taluni più efficaci di altri. Tra questi, abbiamo sostituito l'ottimizzatore con uno [adam](Modello_convolutivo_adam.ipynb).\n",
    "\n",
    "Siamo passati da un modello che presentava un adattamento eccessivo dopo poche epoche a un modello robusto, in grado di generalizzare e con un'accuratezza notevole. \n",
    "\n",
    "Terminato ciò abbiamo riprodotto un [modello convolutivo complesso](Modello_convolutivo_complesso.ipynb) con architettura VggVox."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed0bcb-6820-4efa-9edd-330e56d4703c",
   "metadata": {},
   "source": [
    "## Risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e6739-c0ee-46b3-a0ea-113f07f94283",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1704f732-e6ba-4e4f-8c47-f7c164d900b3",
   "metadata": {},
   "source": [
    "## Confronti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49987deb-d754-4fac-989e-bac3a1eefdf7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b4b07f9-3b64-4076-82fd-f472e57faec2",
   "metadata": {},
   "source": [
    "## Citazioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf9b17-c422-484d-aa86-18af5d3b77b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
