{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751b9f51-b2f9-4758-be26-88a70f1e6d04",
   "metadata": {},
   "source": [
    "# Valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52578c8a-1c8c-45fb-a305-0d85a20ad359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tabulate import tabulate\n",
    "import os \n",
    "\n",
    "import onnxruntime as rt\n",
    "import tf2onnx\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b8aee5-bb5d-498f-a194-3991a98d6758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12933 files belonging to 30 classes.\n",
      "Using 7760 files for training.\n",
      "Using 5173 files for validation.\n"
     ]
    }
   ],
   "source": [
    "headers = [\"Modello\", \"Dataset\", \"Rumore\", \"Accuratezza\"]\n",
    "\n",
    "train_ds, validation_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory='../reduced_dataset/dataset/audio',\n",
    "    validation_split=0.4, # stiamo mettendo da parte il 40% del dataset, che sarà suddiviso in validation set e test set\n",
    "    shuffle=True,\n",
    "    subset='both', # necessario se stiamo utilizzando validation_split (se no darebbe errore)\n",
    "    seed=0 # necessario se stiamo utilizzando sia shuffle che validation_split (se no darebbe errore)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b96fc3a5-0195-4e1b-a62b-ecfec645826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(audio, labels):\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    return audio, labels\n",
    "\n",
    "train_ds = train_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "validation_ds = validation_ds.map(squeeze, tf.data.AUTOTUNE)\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "    spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    return spectrogram[..., tf.newaxis]\n",
    "\n",
    "def get_spectrogram_dataset(dataset):\n",
    "    dataset = dataset.map(lambda x, y: (get_spectrogram(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "train_ds = get_spectrogram_dataset(train_ds)\n",
    "validation_ds = get_spectrogram_dataset(validation_ds)\n",
    "\n",
    "val_ds = validation_ds.take(validation_ds.cardinality() // 2) # ho cambiato nome del validation_ds in modo tale da non creare problemi con l'istruzione seguente\n",
    "test_ds = validation_ds.skip(validation_ds.cardinality() // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46883168-0326-45c5-a6c9-f2f7f8018707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_onnx_model(path_model_onnx, test_ds):\n",
    "    # il fatto che sia suddiviso in batch mi crea problemi, perciò lo risolvo togliendoli\n",
    "    test_ds = test_ds.unbatch()\n",
    "    \n",
    "    # carico il modello utilizzando il file onnx\n",
    "    m = rt.InferenceSession(path_model_onnx)\n",
    "    \n",
    "    # trasformo il dataset in array numpy\n",
    "    spectrogram_np = np.array([spectrogram.numpy() for spectrogram, _ in test_ds])\n",
    "    labels_np = np.array([label.numpy() for _, label in test_ds])\n",
    "    \n",
    "    # eseguo le predizione del modello\n",
    "    pred_onnx = m.run(None, {'input': spectrogram_np})\n",
    "    # ottengo la predizione corretta\n",
    "    predictions = np.argmax(pred_onnx[0], axis=1)\n",
    "    # computo la accuratezza\n",
    "    accuracy = np.mean(predictions == labels_np)\n",
    "    # stampo l'accuratezza\n",
    "    print(f\"Accuratezza: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e06cebbe-20c1-4a68-aa9b-bbf2dfedd9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(dataset=\"all\", optimizer=\"rmsprop\", noise=False):\n",
    "    path = f\"bestmodels/noise/{optimizer}/\" if noise else f\"bestmodels/{optimizer}/\"\n",
    "    path = path if dataset == \"all\" else path+dataset+\"/\"\n",
    "    for model in os.listdir(path):\n",
    "        if model.endswith(\"onnx\"):\n",
    "            evaluate_onnx_model(path+model, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b2122-a320-4817-87e3-5db55de0548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_table(dataset = \"spectrogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff2e0c-cb31-4d19-b9cc-88f897b8ba70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d4125-db0a-42be-b527-bdac5a5af750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
