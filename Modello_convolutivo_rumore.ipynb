{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f04ec7bd-c735-4208-805b-43a57530b57b",
   "metadata": {},
   "source": [
    "# Modello convolutivo di base - Dataset rumore\n",
    "In precedenza abbiamo visto l'addestramento del nostro modello convolutivo di base sul dataset originale, e dopodiché abbiamo parlato della data augmentation.\n",
    "\n",
    "AGGIORNARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a9967-3d0b-4b09-b2c0-0ca41f1f9c7a",
   "metadata": {},
   "source": [
    "Come al solito, implementiamo le librerie funzioni necessarie. Inoltre otteniamo il nuovo dataset attraverso le tecniche viste nel notebook relativo alla data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61875295-f250-4514-bf06-b9e7cae539c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "# per il modello con estensione .onnx\n",
    "import onnxruntime as rt\n",
    "import tf2onnx\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d43246-a2da-4049-a836-5a7fab23ec38",
   "metadata": {},
   "source": [
    "## Classe per la conversione del dataset\n",
    "Questa classe l'abbiamo già vista implementata nel notebook riguardante l'addestramento dei filterbanks e degli mfcc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b640e0-93f0-431e-9e02-c67d45016b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConverter:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def convert(self, option):\n",
    "        available_options = ['spettrogrammi', 'filterbanks', 'mfcc']\n",
    "        \n",
    "        if option == available_options[0]:\n",
    "            return self.get_spectrogram_dataset()\n",
    "        elif option == available_options[1]:\n",
    "            return self.get_filterbanks_dataset()\n",
    "        elif option == available_options[2]:\n",
    "            return self.get_mfcc_dataset()\n",
    "        else:\n",
    "            raise ValueError(f\"Opzione non disponibile: inserire una delle seguenti opzioni: {available_options}\")\n",
    "    \n",
    "    # INIZIO SPETTROGRAMMI\n",
    "    def squeeze(self, audio, labels):\n",
    "        audio = tf.squeeze(audio, axis=-1)\n",
    "        return audio, labels\n",
    "    \n",
    "    def get_spectrogram(self, waveform):\n",
    "    # applichiamo la short-time Fourier transorm\n",
    "        spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n",
    "        spectrogram = tf.abs(spectrogram)\n",
    "        \n",
    "        return spectrogram[..., tf.newaxis]\n",
    "    \n",
    "    def get_spectrogram_dataset(self):\n",
    "        # squeeze\n",
    "        self.dataset = self.dataset.map(self.squeeze, tf.data.AUTOTUNE)\n",
    "        self.dataset = self.dataset.map(lambda x, y: (self.get_spectrogram(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        return self.dataset\n",
    "\n",
    "    # FINE SPETTROGRAMMI\n",
    "\n",
    "    def convert_to_numpy(self, dataset):\n",
    "        audio_data = []\n",
    "        labels = []\n",
    "    \n",
    "        dataset = dataset.unbatch()\n",
    "        \n",
    "        for audio, label in dataset:\n",
    "            audio_data.append(audio.numpy())  # Assuming audio is a tensor, convert to numpy array\n",
    "            labels.append(label.numpy())      # Assuming label is a tensor, convert to numpy array\n",
    "        \n",
    "        audio_data = np.array(audio_data)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        return audio_data, labels\n",
    "    \n",
    "    # INIZIO FILTERBANKS\n",
    "    def makeHamming(self, M):\n",
    "        R = (( M - 1 ) / 2 , M / 2)[M % 2 == 0]\n",
    "        w = (np.hamming(M), np.hamming(M + 1))[M % 2 == 0]\n",
    "        if M % 2 != 0:\n",
    "            w[0] = w[0]/2\n",
    "            w[M-1] = w[M-1]/2\n",
    "        else:\n",
    "            w = w[:M]\n",
    "    \n",
    "        return w\n",
    "\n",
    "    def hztomel(self, hz):\n",
    "        return (2595 * np.log10(1 + hz / 700))\n",
    "\n",
    "    def meltohz(self, mel):\n",
    "        return (700 * (10**(mel / 2595) - 1))\n",
    "\n",
    "    def compute_filterbanks(self, audios_np, pre_emphasis=0.97, sample_rate=16000, frame_size=0.025, frame_stride=0.01, NFFT=512, nfilt=40):\n",
    "        filterbanks_np = []\n",
    "        \n",
    "        for samples in audios_np:\n",
    "            emphasized_audio = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
    "            audio_length = len(emphasized_audio)\n",
    "    \n",
    "            frame_length, frame_step = int(frame_size * sample_rate), int(frame_stride * sample_rate)\n",
    "    \n",
    "            num_frames = int(np.ceil(float(np.abs(audio_length - frame_length)) / frame_step))\n",
    "    \n",
    "            pad_audio_length = num_frames * frame_step + frame_length\n",
    "            z = np.zeros((pad_audio_length - audio_length))\n",
    "            pad_audio = np.append(emphasized_audio, z)\n",
    "    \n",
    "            indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "            frames = pad_audio[indices.astype(np.int32, copy=False)]\n",
    "    \n",
    "            # Usiamo la funzione di Hamming\n",
    "            hamming_window = self.makeHamming(frame_length)\n",
    "    \n",
    "            mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitudo della FFT\n",
    "            pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))\n",
    "    \n",
    "            # convertiamo hz in mel\n",
    "            low_freq_mel = self.hztomel(0)\n",
    "            high_freq_mel = self.hztomel(sample_rate / 2)\n",
    "    \n",
    "            mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "            hz_points = self.meltohz(mel_points) \n",
    "    \n",
    "            bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "    \n",
    "            fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    \n",
    "            for m in range(1, nfilt + 1):\n",
    "                f_m_minus = int(bin[m - 1])\n",
    "                f_m = int(bin[m])\n",
    "                f_m_plus = int(bin[m + 1])\n",
    "    \n",
    "                for k in range(f_m_minus, f_m):\n",
    "                    fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "                for k in range(f_m, f_m_plus):\n",
    "                    fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    \n",
    "            # in questo momento invece calcoliamo i filter banks per i segmenti di audio, utilizzando i filtri triangolari appena creati\n",
    "            filter_banks = np.dot(pow_frames, fbank.T)\n",
    "            filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "            filter_banks = 20 * np.log10(filter_banks)\n",
    "    \n",
    "            filterbanks_np.append(filter_banks)\n",
    "        \n",
    "        return np.array(filterbanks_np)\n",
    "    \n",
    "    def get_filterbanks_dataset(self): \n",
    "        audios, labels = self.convert_to_numpy(self.dataset)\n",
    "\n",
    "        filterbanks = self.compute_filterbanks(audios)\n",
    "        filterbanks = np.expand_dims(filterbanks, axis=-1)\n",
    "\n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((filterbanks, labels))\n",
    "        self.dataset = self.dataset.batch(32)\n",
    "        \n",
    "        return self.dataset\n",
    "    # FINE FILTERBANKS\n",
    "\n",
    "    # INIZIO MFCC\n",
    "    def compute_mfcc(self, filter_banks, num_ceps=12, cep_lifter=22):\n",
    "        mfcc_np = []\n",
    "        \n",
    "        for f in filter_banks:\n",
    "            mfcc = scipy.dct(f, type=2, axis=1, norm='ortho')[:, 1 : (num_ceps + 1)]\n",
    "\n",
    "            (nframes, ncoeff) = mfcc.shape\n",
    "            n = np.arange(ncoeff)\n",
    "            \n",
    "            lift = 1 + (cep_lifter / 2) * np.sin(np.pi * n / cep_lifter)\n",
    "            mfcc *= lift\n",
    "\n",
    "            mfcc_np.append(mfcc)\n",
    "        \n",
    "        return np.array(mfcc_np)\n",
    "    \n",
    "    \n",
    "    def get_mfcc_dataset(self):\n",
    "        audios, labels = self.convert_to_numpy(self.dataset)\n",
    "\n",
    "        filterbanks = self.compute_filterbanks(audios)\n",
    "        mfcc = self.compute_mfcc(filterbanks)\n",
    "        \n",
    "        mfcc = np.expand_dims(mfcc, axis=-1)\n",
    "        \n",
    "        self.dataset = tf.data.Dataset.from_tensor_slices((mfcc, labels))\n",
    "        self.dataset = self.dataset.batch(32)\n",
    "        \n",
    "        return self.dataset\n",
    "    \n",
    "    # FINE MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a091f254-b9dd-4ad4-b590-ddf13bbb9918",
   "metadata": {},
   "source": [
    "## Importazione funzioni ulteriori\n",
    "In aggiunta alla classe per la conversione del dataset, ci serviremo delle seguenti funzioni: \n",
    "- `convert_history_to_csv` - salva l'history dell'addestramento in un csv\n",
    "- `convert_model_to_onnx` - converte il miglior modello **.keras** in **.onnx**\n",
    "- `create_train_val_plot` - visualizza l'andamento dell'accuratezza e della perdita durante l'addestramento\n",
    "- `evaluate_onnx_model` - valuta il modello sui dati di test con il modello **.onnx**\n",
    "- `create_distribution_plot` - visualizza la distribuzione di accuratezza delle classi su un'immagine casuale\n",
    "\n",
    "Anch'esse sono già state viste, perciò non ci soffermeremo troppo su di esse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecfaeeda-0c38-47ef-a635-d8a82002c0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_history_to_csv(model_history):\n",
    "    # converto la history del dataframe in un dataframe pandas\n",
    "    model_history_df = pd.DataFrame(model_history.history)\n",
    "    # cambio il nome dell'indice e lo imposto a partire da 1\n",
    "    model_history_df.index = range(1, len(model_history_df) + 1)\n",
    "    model_history_df.index.name = \"epochs\"\n",
    "\n",
    "    return model_history_df\n",
    "\n",
    "def convert_model_to_onnx(model_path, input_shape):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    onnx_path = model_path.split(\".\")[:-1] + [\"onnx\"]\n",
    "    onnx_path = \".\".join(onnx_path)\n",
    "    \n",
    "    input_signature = [tf.TensorSpec((None, *input_shape), tf.float32, name=\"input\")]\n",
    "    onnx_model, _ = tf2onnx.convert.from_keras(model, input_signature=input_signature)\n",
    "\n",
    "    onnx.save(onnx_model, onnx_path)\n",
    "\n",
    "def create_train_val_plot(history, overfit=True):\n",
    "    # definisco numero epoche\n",
    "    epochs = range(1, len(history['accuracy']) + 1)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = history['accuracy']\n",
    "    val_accuracy = history['val_accuracy']\n",
    "\n",
    "    fig_acc, ax_acc = plt.subplots()\n",
    "\n",
    "    # linea di base\n",
    "    ax_acc.plot(epochs, [0.5 for x in range(len(epochs))], color=\"lightgray\", linestyle=\"--\", label=\"Base\")\n",
    "    # linee di accuratezza\n",
    "    ax_acc.plot(epochs, accuracy, color=\"slategray\", label=\"Accuratezza in addestramento\")\n",
    "    ax_acc.plot(epochs, val_accuracy, color=\"indianred\", label=\"Accuratezza in validazione\")\n",
    "    # miglior accuratezza validazione\n",
    "    ax_acc.axhline(y=max(val_accuracy), c='indianred', alpha=0.7, linestyle='--')\n",
    "    ax_acc.text(len(epochs) * 1.07, max(val_accuracy), round(max(val_accuracy), 2), fontsize=9, fontweight=\"bold\", horizontalalignment=\"left\", verticalalignment=\"center\")\n",
    "    # nascondo limiti plot\n",
    "    ax_acc.spines[\"right\"].set_visible(False)\n",
    "    ax_acc.spines[\"top\"].set_visible(False)\n",
    "    ax_acc.spines[\"left\"].set_visible(False)\n",
    "    # definizione label e titolo + legenda\n",
    "    ax_acc.set_title(\"Accuratezza in addestramento e validazione\")\n",
    "    ax_acc.set_xlabel(\"Epoche\")\n",
    "    ax_acc.set_ylabel(\"Accuratezza (%)\")\n",
    "    ax_acc.legend()\n",
    "\n",
    "    # Loss\n",
    "    loss = history['loss']\n",
    "    val_loss = history['val_loss']\n",
    "\n",
    "    fig_loss, ax_loss = plt.subplots()\n",
    "\n",
    "    # linee di loss\n",
    "    ax_loss.plot(epochs, loss, color=\"slategray\", label=\"Perdita in addestramento\")\n",
    "    ax_loss.plot(epochs, val_loss, color=\"indianred\", label=\"Perdita in validazione\")\n",
    "    # miglior loss validazione\n",
    "    ax_loss.axhline(y=min(val_loss), c='indianred', alpha=0.7, linestyle='--')\n",
    "    ax_loss.text(len(epochs) * 1.07, min(val_loss), round(min(val_loss), 2), fontsize=9, fontweight=\"bold\", horizontalalignment=\"left\", verticalalignment=\"center\")\n",
    "    # nascondo limiti plot\n",
    "    ax_loss.spines[\"right\"].set_visible(False)\n",
    "    ax_loss.spines[\"top\"].set_visible(False)\n",
    "    ax_loss.spines[\"left\"].set_visible(False)\n",
    "    # definizione label e titolo + legenda\n",
    "    ax_loss.set_title(\"Perdita in addestramento e validazione\")\n",
    "    ax_loss.set_xlabel(\"Epoche\")\n",
    "    ax_loss.set_ylabel(\"Perdita\")\n",
    "\n",
    "    handles, _ = ax_loss.get_legend_handles_labels()\n",
    "\n",
    "    if overfit:\n",
    "        # area overfit\n",
    "        rect = mpatches.Rectangle((np.argmin(val_loss) + 1, 0), width=100 - np.argmin(val_loss), height=max(max(loss), max(val_loss)), color='lightcoral', alpha=0.3)\n",
    "        ax_loss.add_patch(rect)\n",
    "        # patch overfit per legenda\n",
    "        overfit = mpatches.Patch(color='indianred', alpha=0.3, label='Area overfit')\n",
    "        handles, _ = ax_loss.get_legend_handles_labels()\n",
    "        handles.append(overfit)\n",
    "\n",
    "    # legenda\n",
    "    ax_loss.legend(handles=handles)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_onnx_model(path_model_onnx, test_ds):\n",
    "    # il fatto che sia suddiviso in batch mi crea problemi, perciò lo risolvo togliendoli\n",
    "    test_ds = test_ds.unbatch()\n",
    "    \n",
    "    # carico il modello utilizzando il file onnx\n",
    "    m = rt.InferenceSession(path_model_onnx)\n",
    "    \n",
    "    # trasformo il dataset in array numpy\n",
    "    spectrogram_np = np.array([spectrogram.numpy() for spectrogram, _ in test_ds], dtype=np.float32)\n",
    "    labels_np = np.array([label.numpy() for _, label in test_ds])\n",
    "    \n",
    "    # eseguo le predizione del modello\n",
    "    pred_onnx = m.run(None, {'input': spectrogram_np})\n",
    "    # ottengo la predizione corretta\n",
    "    predictions = np.argmax(pred_onnx[0], axis=1)\n",
    "    # computo la accuratezza\n",
    "    accuracy = np.mean(predictions == labels_np)\n",
    "    # stampo l'accuratezza\n",
    "    print(f\"Accuratezza: {accuracy:.3f}\")\n",
    "\n",
    "def create_distribution_plot(model, audio, label):\n",
    "    prediction = model(audio) # otteniamo la predizione del modello sull'audio preso in input\n",
    "\n",
    "    random_audio_index = np.random.randint(0, len(prediction) - 1) # abbiamo notato che mostravamo sempre la classe no, magari così cambia un po' le classi che mostriamo\n",
    "\n",
    "    softmax_pred = tf.nn.softmax(prediction[random_audio_index]) # dobbiamo applicare nuovamente la softmax in modo tale da avere dei dati leggibili\n",
    "    pred_label = np.argmax(softmax_pred) # ci salviamo l'indice della label predetta dal nostro modello, quella con la probabilità più alta\n",
    "\n",
    "    colors = ['indianred'] * len(label_names) # creiamo un vettore di lunghezza 30 (la quantità delle nostre label) colorate di rosso\n",
    "\n",
    "    if pred_label == label[random_audio_index].numpy(): # se la label che abbiamo predetto è corretta, allora la coloriamo di verde\n",
    "        colors[pred_label] = 'seagreen'\n",
    "    else: # se la label che abbiamo predetto è sbagliata, allora coloriamo di verde la corretta e di gialla la predetta\n",
    "        colors[pred_label] = 'khaki'\n",
    "        colors[label[random_audio_index].numpy()] = 'seagreen'\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.bar(label_names, softmax_pred.numpy(), color=colors)\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # dichiariamo i colori della legenda\n",
    "    other_values_patch = mpatches.Patch(color='indianred', label='Altre classi')\n",
    "    pred_value_patch = mpatches.Patch(color='khaki', label='Classe predetta')\n",
    "    correct_value_patch = mpatches.Patch(color='green', label='Classe corretta')\n",
    "\n",
    "    # disegniamo la legenda con i colori necessari\n",
    "    if pred_label == label[random_audio_index].numpy(): # se abbiamo predetto correttamente la label, non è necessario mostrare il colore giallo (non è presente nel plot)\n",
    "        plt.legend(handles=[other_values_patch, correct_value_patch])\n",
    "    else:\n",
    "        plt.legend(handles=[other_values_patch, pred_value_patch, correct_value_patch])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9688413-df52-4c19-8a12-0f7837e0ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12933 files belonging to 30 classes.\n",
      "Using 7760 files for training.\n",
      "Using 5173 files for validation.\n",
      "Found 14 files belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds, validation_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory ='../reduced_dataset/dataset/audio',\n",
    "    validation_split=0.4, # stiamo mettendo da parte il 40% del dataset, che sarà suddiviso in validation set e test set\n",
    "    shuffle=True,\n",
    "    subset='both', # necessario se stiamo utilizzando validation_split\n",
    "    seed=0 # necessario se stiamo utilizzando sia shuffle che validation_split \n",
    ")\n",
    "audio_label_names = train_ds.class_names\n",
    "\n",
    "noise_ds = tf.keras.utils.audio_dataset_from_directory(\n",
    "    directory='../noise_dataset',\n",
    "    batch_size = 1\n",
    ")\n",
    "\n",
    "noise_label_names = noise_ds.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb4272-a843-4d2f-8b26-01f6e582110c",
   "metadata": {},
   "source": [
    "## Applicazione tecniche data augmentation\n",
    "Abbiamo già visto l'utilizzo delle tecniche che andremo a applicare in questo momento, perciò non ci soffermeremo sulla loro spiegazione ma descriveremo i passaggi molto sinteticamnete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e13c5bd-0a66-42bb-bbc5-cdfa8e34ee1c",
   "metadata": {},
   "source": [
    "Tagliamo gli audio del dataset rumoroso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59e0486b-51e0-4486-a740-63d8af3f324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_audios(dataset, length):\n",
    "    new_audios = [] # inizializziamo una lista dove inseriremo i nostri audio tagliati\n",
    "    labels = []\n",
    "    \n",
    "    # iteriamo nel dataset\n",
    "    for audio, label in dataset:\n",
    "        # tagliamo l'audio ad un secondo e lo appendiamo a una lista \n",
    "        labels.append(label.numpy())\n",
    "        \n",
    "        audio = tf.reshape(audio, [-1])[np.shape(audio)[1]//2:np.shape(audio)[1]//2 + length]\n",
    "        new_audios.append(audio.numpy()) # convertiamo in array per poterli modificare\n",
    "        \n",
    "    return new_audios, labels\n",
    "\n",
    "cut_noise_audios, cut_noise_labels = cut_audios(noise_ds, 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2a051b-d535-465e-9f12-cdba69040dd4",
   "metadata": {},
   "source": [
    "Creiamo il nuovo dataset, unendo l'originale con quello rumoroso. L'output che otteneremo è una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0303144-3fcf-4a78-852a-5bce7c71c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_audios(original_audios, noise_audios):\n",
    "    mixed_dataset = [] # inizializziamo la lista dove inseriremo gli audio uniti al rumore\n",
    "\n",
    "    original_audios = original_audios.unbatch() # il nostro training set ha una batch_size di 32, per rendere il processo più semplice unbatchiamo\n",
    "    \n",
    "    # per ogni audio del dataset originale\n",
    "    for audio, label in original_audios:    \n",
    "        audio = np.squeeze(audio, axis=-1) # rimuoviamo l'ultima asse inutile (quella dei canali)\n",
    "        \n",
    "        # Scegliamo in modo randomico un audio dalla lista degli audio rumorosi\n",
    "        noise_sample = random.choice(noise_audios)\n",
    "\n",
    "        # calcolo l'ampiezza massima dell'audio rumoroso\n",
    "        max_amplitude_audio = np.max(np.abs(audio))\n",
    "        max_amplitude_noise = np.max(np.abs(noise_sample))\n",
    "        # calcolo un noise factor che varia a seconda dell'ampiezza massima di entrambi gli audio\n",
    "        noise_factor = max_amplitude_audio / max_amplitude_noise\n",
    "        noise_factor = min(noise_factor, 1.0)\n",
    "        \n",
    "        noise_sample = noise_sample * noise_factor\n",
    "        \n",
    "        mixed_audio = audio + noise_sample # uniamo l'audio original al rumore\n",
    "\n",
    "        # aggiungiamo l'audio con noise alla lista\n",
    "        mixed_dataset.append((mixed_audio, label))\n",
    "\n",
    "    return mixed_dataset\n",
    "\n",
    "mixed_train_list = mix_audios(train_ds, cut_noise_audios)\n",
    "mixed_val_list = mix_audios(validation_ds, cut_noise_audios)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbabaa8-ffed-484a-bb02-4cb05ecb413c",
   "metadata": {},
   "source": [
    "Convertiamo la lista in un dataset di tensori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e184740-9cbf-44be-a4e0-9bc9981542f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mixed_ds(dataset_list):\n",
    "    audio_data = [tf.convert_to_tensor(audio, dtype=tf.float32) for audio, label in dataset_list]\n",
    "    labels = [label for _, label in dataset_list]\n",
    "\n",
    "    audio_data = tf.expand_dims(audio_data, axis=-1)\n",
    "    \n",
    "    mixed_train_ds = tf.data.Dataset.from_tensor_slices((audio_data, labels))\n",
    "    mixed_train_ds = mixed_train_ds.batch(32)\n",
    "    return mixed_train_ds\n",
    "\n",
    "mixed_train_ds = create_mixed_ds(mixed_train_list)\n",
    "mixed_validation_ds = create_mixed_ds(mixed_val_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cecf9f-9816-4500-a1ca-4813022cc392",
   "metadata": {},
   "source": [
    "## Suddivisione dataset\n",
    "Ottenuto il dataset rumoroso possiamo procedere a convertirlo e suddividerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5e5cbfd-0115-4ce0-a4db-ede3eff06ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_train_ds = DatasetConverter(mixed_train_ds)\n",
    "mixed_train_ds = mixed_train_ds.convert(\"spettrogrammi\")\n",
    "\n",
    "mixed_validation_ds = DatasetConverter(mixed_validation_ds)\n",
    "mixed_validation_ds = mixed_validation_ds.convert(\"spettrogrammi\")\n",
    "\n",
    "mixed_val_ds = mixed_validation_ds.take(mixed_validation_ds.cardinality() // 2) # ho cambiato nome del validation_ds in modo tale da non creare problemi con l'istruzione seguente\n",
    "mixed_test_ds = mixed_validation_ds.skip(mixed_validation_ds.cardinality() // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb96353-f248-4b02-b676-f053e652635a",
   "metadata": {},
   "source": [
    "Adesso siamo pronti per addestrare i modelli su un dataset rumoroso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1b5347-be10-4830-be2f-12bb148414f4",
   "metadata": {},
   "source": [
    "## Addestramento\n",
    "Come già detto, le reti neurali convolutive che andremo a trattare sono le stesse presenti nel notebook del modello convolutivo base. Stiamo addestrando gli stessi modelli in modo da poter confrontare le loro prestazioni in un notebook a parte.\n",
    "\n",
    "Iniziamo con il primo modello base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff5b5fd-08fa-49cd-b53c-cf5bc0c7b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_model():\n",
    "    # Dichiariamo lo shape dell'input, ridimensioniamo le immagini, e normalizziamo\n",
    "    inputs = tf.keras.Input(shape=(124, 129, 1), name=\"inputs\")\n",
    "    x = tf.keras.layers.Normalization(name=\"normalizzazione\")(inputs)\n",
    "\n",
    "    # Blocco di apprendimento delle caratteristiche\n",
    "    conv2D_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_1\")(x)\n",
    "    MaxPooling2D_1 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_1\")(conv2D_1)\n",
    "    conv2D_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_2\")(MaxPooling2D_1)\n",
    "    MaxPooling2D_2 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_2\")(conv2D_2)\n",
    "    conv2D_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_3\")(MaxPooling2D_2)\n",
    "    MaxPooling2D_3 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_3\")(conv2D_3)\n",
    "    conv2D_4 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_4\")(MaxPooling2D_3)\n",
    "    MaxPooling2D_4 = tf.keras.layers.MaxPooling2D(pool_size=2, name=\"MaxPooling2D_4\")(conv2D_4)\n",
    "    conv2D_5 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv2D_5\")(MaxPooling2D_4)\n",
    "\n",
    "    # Blocco di classificazione\n",
    "    Flatten = tf.keras.layers.Flatten(name=\"Flatten\")(conv2D_5)\n",
    "    outputs = tf.keras.layers.Dense(30, activation=\"softmax\", name=\"dense_output\")(Flatten)\n",
    "\n",
    "    # Modello\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb40005-9589-481e-bc6e-d85543959e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "243/243 [==============================] - 6s 12ms/step - loss: 2.9934 - accuracy: 0.1598 - val_loss: 2.6077 - val_accuracy: 0.2755\n",
      "Epoch 2/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 1.9266 - accuracy: 0.4508 - val_loss: 1.6388 - val_accuracy: 0.5243\n",
      "Epoch 3/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 1.1676 - accuracy: 0.6559 - val_loss: 1.3985 - val_accuracy: 0.5976\n",
      "Epoch 4/30\n",
      "243/243 [==============================] - 3s 11ms/step - loss: 0.7750 - accuracy: 0.7684 - val_loss: 1.2674 - val_accuracy: 0.6389\n",
      "Epoch 5/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.5540 - accuracy: 0.8378 - val_loss: 1.3004 - val_accuracy: 0.6929\n",
      "Epoch 6/30\n",
      "243/243 [==============================] - 3s 11ms/step - loss: 0.3902 - accuracy: 0.8799 - val_loss: 1.6953 - val_accuracy: 0.6701\n",
      "Epoch 7/30\n",
      "243/243 [==============================] - 3s 11ms/step - loss: 0.2980 - accuracy: 0.9115 - val_loss: 1.6892 - val_accuracy: 0.7438\n",
      "Epoch 8/30\n",
      "243/243 [==============================] - 2s 10ms/step - loss: 0.2507 - accuracy: 0.9269 - val_loss: 1.7401 - val_accuracy: 0.7350\n",
      "Epoch 9/30\n",
      "243/243 [==============================] - 2s 10ms/step - loss: 0.2193 - accuracy: 0.9357 - val_loss: 1.7254 - val_accuracy: 0.7218\n",
      "Epoch 10/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.2018 - accuracy: 0.9430 - val_loss: 2.1639 - val_accuracy: 0.7303\n",
      "Epoch 11/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1743 - accuracy: 0.9506 - val_loss: 1.7227 - val_accuracy: 0.7427\n",
      "Epoch 12/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1569 - accuracy: 0.9581 - val_loss: 2.1138 - val_accuracy: 0.7469\n",
      "Epoch 13/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1512 - accuracy: 0.9588 - val_loss: 2.6374 - val_accuracy: 0.7299\n",
      "Epoch 14/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1512 - accuracy: 0.9604 - val_loss: 2.9611 - val_accuracy: 0.7215\n",
      "Epoch 15/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1618 - accuracy: 0.9634 - val_loss: 2.1217 - val_accuracy: 0.7311\n",
      "Epoch 16/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1360 - accuracy: 0.9679 - val_loss: 2.3446 - val_accuracy: 0.7450\n",
      "Epoch 17/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1176 - accuracy: 0.9682 - val_loss: 3.2498 - val_accuracy: 0.7365\n",
      "Epoch 18/30\n",
      "243/243 [==============================] - 3s 11ms/step - loss: 0.1328 - accuracy: 0.9689 - val_loss: 2.9400 - val_accuracy: 0.7508\n",
      "Epoch 19/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1314 - accuracy: 0.9687 - val_loss: 2.8126 - val_accuracy: 0.7492\n",
      "Epoch 20/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1281 - accuracy: 0.9705 - val_loss: 2.6530 - val_accuracy: 0.7438\n",
      "Epoch 21/30\n",
      "243/243 [==============================] - 3s 11ms/step - loss: 0.1289 - accuracy: 0.9733 - val_loss: 2.6043 - val_accuracy: 0.7535\n",
      "Epoch 22/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1340 - accuracy: 0.9709 - val_loss: 3.6528 - val_accuracy: 0.7045\n",
      "Epoch 23/30\n",
      "243/243 [==============================] - 2s 10ms/step - loss: 0.1122 - accuracy: 0.9728 - val_loss: 3.4371 - val_accuracy: 0.7504\n",
      "Epoch 24/30\n",
      "243/243 [==============================] - 2s 10ms/step - loss: 0.1338 - accuracy: 0.9745 - val_loss: 4.3595 - val_accuracy: 0.7130\n",
      "Epoch 25/30\n",
      "243/243 [==============================] - 3s 10ms/step - loss: 0.1351 - accuracy: 0.9728 - val_loss: 3.3603 - val_accuracy: 0.7380\n",
      "Epoch 26/30\n",
      "243/243 [==============================] - 3s 11ms/step - loss: 0.1301 - accuracy: 0.9737 - val_loss: 4.2269 - val_accuracy: 0.7303\n",
      "Epoch 27/30\n",
      "243/243 [==============================] - 3s 11ms/step - loss: 0.1325 - accuracy: 0.9772 - val_loss: 5.7758 - val_accuracy: 0.7068\n",
      "Epoch 28/30\n",
      "238/243 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9741"
     ]
    }
   ],
   "source": [
    "basic_model = get_basic_model()\n",
    "basic_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "\n",
    "basic_model_callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=\"bestmodels/noise/rmsprop/basic_model.keras\", save_best_only=True, monitor=\"val_loss\")]\n",
    "basic_model_history = basic_model.fit(mixed_train_ds, epochs=30, validation_data=mixed_val_ds, callbacks=basic_model_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10008b53-beba-44dc-b2b1-b6fd22843ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
